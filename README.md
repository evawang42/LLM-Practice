# LLM-Practice
## Task 1 â€” Streaming chat with Ollama (Python, Async)
This task implements a minimal **async streaming** chat function against a local Ollama server

`config.py` example:
```py
OLLAMA_URL = "http://127.0.0.1:11434"
```

## Reference
- https://github.com/ollama/ollama-python